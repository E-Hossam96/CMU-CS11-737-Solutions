
1. Bilingual Baselines
======================

az_en
-----

- evaluating test set
    BLEU = 3.01 12.5/3.7/2.1/0.9 (BP = 1.000 ratio = 1.623 hyp_len = 31406 ref_len = 19356)
    COMET = -1.4159


- evaluating valid set
    BLEU = 2.67 12.1/3.4/1.8/0.7 (BP = 1.000 ratio = 1.611 hyp_len = 20932 ref_len = 12997)
    COMET = -1.4303


en_az
-----

- evaluating test set
    BLEU = 20.21 34.3/22.8/17.8/12.7 (BP = 0.986 ratio = 0.987 hyp_len = 17278 ref_len = 17513)
    COMET = -1.2864


- evaluating valid set
    BLEU = 21.12 34.1/23.7/18.5/13.2 (BP = 1.000 ratio = 1.017 hyp_len = 12447 ref_len = 12239)
    COMET = -1.3057


be_en
-----

- evaluating test set
    BLEU = 4.67 18.1/5.9/3.0/1.5 (BP = 1.000 ratio = 1.825 hyp_len = 27498 ref_len = 15066)
    COMET = -1.3589


- evaluating valid set
    BLEU = 4.66 18.7/5.9/3.0/1.4 (BP = 1.000 ratio = 1.853 hyp_len = 8712 ref_len = 4701)
    COMET = -1.3841


en_be
-----

- evaluating test set
    BLEU = 11.46 23.2/12.8/9.1/6.4 (BP = 1.000 ratio = 1.827 hyp_len = 28079 ref_len = 15372)
    COMET = -1.3506


- evaluating valid set
    BLEU = 15.40 30.6/17.4/12.3/8.5 (BP = 1.000 ratio = 1.423 hyp_len = 7688 ref_len = 5402)
    COMET = -1.3057



2. Multilingual Training
========================

aztr_en
-------

- evaluating test set
    BLEU = 14.31 40.3/18.0/10.0/5.8 (BP = 1.000 ratio = 1.130 hyp_len = 21963 ref_len = 19438)
    COMET = -0.2684


- evaluating valid set
    BLEU = 17.00 43.5/21.0/12.4/7.4 (BP = 1.000 ratio = 1.115 hyp_len = 14673 ref_len = 13165)
    COMET = -0.2378



en_aztr
-------

- evaluating test set
    BLEU = 5.92 30.7/9.2/3.4/1.3 (BP = 1.000 ratio = 1.087 hyp_len = 14226 ref_len = 13092)
    COMET = -0.0691


- evaluating valid set
    BLEU = 8.24 32.8/11.6/5.1/2.4 (BP = 1.000 ratio = 1.100 hyp_len = 9952 ref_len = 9044)
    COMET = -0.0116


beru_en
-------

- evaluating test set
    BLEU = 18.90 44.9/22.9/14.0/8.9 (BP = 1.000 ratio = 1.134 hyp_len = 17079 ref_len = 15066)
    COMET = -0.3816


- evaluating valid set
    BLEU = 20.30 45.2/24.2/15.4/10.1 (BP = 1.000 ratio = 1.187 hyp_len = 5582 ref_len = 4701)
    COMET = -0.3545


en_beru
-------

- evaluating test set
    BLEU = 9.40 36.0/13.4/6.1/2.6 (BP = 1.000 ratio = 1.176 hyp_len = 14174 ref_len = 12052)
    COMET = -0.4756


- evaluating valid set
    BLEU = 12.02 39.1/15.8/8.1/4.2 (BP = 1.000 ratio = 1.078 hyp_len = 4485 ref_len = 4162)
    COMET = -0.3303




3. Finetuning Pretrained Multilingual Models
============================================


az_en
-----

- evaluating test set
    BLEU = 17.56 47.9/22.2/12.8/7.6 (BP = 0.979 ratio = 0.979 hyp_len = 19037 ref_len = 19438)
    COMET = -0.0318


- evaluating valid set
    BLEU = 20.69 50.1/25.4/15.3/9.4 (BP = 1.000 ratio = 1.002 hyp_len = 13195 ref_len = 13165)
    COMET = -0.0419


en_az
-----

- evaluating test set
    BLEU = 7.25 33.9/10.8/4.2/1.8 (BP = 1.000 ratio = 1.059 hyp_len = 13861 ref_len = 13092)
    COMET = 0.0368


- evaluating valid set
    BLEU = 9.60 36.3/13.7/6.1/2.8 (BP = 1.000 ratio = 1.057 hyp_len = 9556 ref_len = 9044)
    COMET = 0.0209


be_en
-----

- evaluating test set
    BLEU = 22.82 51.6/27.7/17.2/11.0 (BP = 1.000 ratio = 1.040 hyp_len = 15671 ref_len = 15066)
    COMET = -0.0674


- evaluating valid set
    BLEU = 24.49 52.1/29.7/19.0/12.3 (BP = 1.000 ratio = 1.110 hyp_len = 5216 ref_len = 4701)
    COMET = 0.0161


en_be
-----

- evaluating test set
    BLEU = 14.84 44.7/20.0/10.2/5.3 (BP = 1.000 ratio = 1.089 hyp_len = 13125 ref_len = 12052)
    COMET = 0.0593


- evaluating valid set
    BLEU = 17.16 47.1/22.3/12.3/7.0 (BP = 0.990 ratio = 0.990 hyp_len = 4122 ref_len = 4162)
    COMET = 0.1284